name: Build and Deploy to EKS

on:
  push:
    branches: [ "main" ] # 當推送到 main 分支時觸發

env:
  DOCKER_USERNAME: neo1202 
  AWS_REGION: us-east-1   
  EKS_CLUSTER_NAME: ride-share-cluster

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    # 1. 登入 Docker Hub
    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ env.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }} # 這裡要去 GitHub Settings 設定 Secrets

    # 2. Build & Push Auth Service
    - name: Build and Push Auth Service
      uses: docker/build-push-action@v4
      with:
        context: ./services/auth
        push: true
        tags: ${{ env.DOCKER_USERNAME }}/auth-service:latest

    # 3. Build & Push Chat Service
    - name: Build and Push Chat Service
      uses: docker/build-push-action@v4
      with:
        context: ./services/chat
        push: true
        tags: ${{ env.DOCKER_USERNAME }}/chat-service:latest

    # 4. Build & Push Frontend
    - name: Build and Push Frontend
      # 在這一步，我們把 Secret 寫入 .env 檔案
      # 這樣 Docker build 的時候，Vite 就能讀到這個變數並打包進去
      run: |
        echo "VITE_GOOGLE_CLIENT_ID=${{ secrets.VITE_GOOGLE_CLIENT_ID }}" > ./frontend/.env
        echo "VITE_API_URL=" >> ./frontend/.env
      # 注意：VITE_API_URL 留空，強制使用相對路徑
    - name: Build and Push Frontend
      uses: docker/build-push-action@v4
      with:
        context: ./frontend
        push: true
        tags: ${{ env.DOCKER_USERNAME }}/frontend-service:latest
    
    # 3. 設定 AWS 憑證
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    # 4. 更新 Kubeconfig (讓 kubectl 可以連到 EKS)
    - name: Update Kubeconfig
      run: aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

    # 5. 部署到 EKS
    # 裡面沒有deploy/k8s/nginx-ingress-install.yaml, 在雲端上我們都直接下載最新的。本地我把webhook, job都註解了才能本地跑
    
    - name: Deploy to EKS
      run: |
        kubectl apply -f deploy/argocd.yaml

        # 1. 先部署基礎設施 (ConfigMap, DB, Redis)
        # 注意：Secret 還是得你自己手動在電腦上跑一次，GitHub 沒辦法幫你
        # ❌ 刪除這行 (ConfigMap 由 Terraform 管理) (Secret 也是由 Terraform 管理，且 GitHub 根本沒這檔案)
        # kubectl apply -f deploy/k8s/configmap.yaml
        # ❌ 已經改用雲端版本的rds, 不用在eks裡面創一個暫時的pg pod了
        # kubectl apply -f deploy/k8s/postgres.yaml

        # 更新：加入ArgoCD以後把redis, auth, chat, frontend, ingress的yaml交給他處理
        # 因為 ArgoCD 會監控 GitHub 的 deploy/k8s 資料夾，自動同步
        # kubectl apply -f deploy/k8s/redis.yaml
        
        # 2. 再部署應用程式 (原本的)
        # kubectl apply -f deploy/k8s/auth.yaml
        # kubectl apply -f deploy/k8s/chat.yaml
        # kubectl apply -f deploy/k8s/frontend.yaml
        
        # 3. 路由
        # kubectl apply -f deploy/k8s/ingress.yaml
        
        # 4. 強制重啟 Pod (這是為了讓 Pod 重新拉取 :latest 的 Image)
        kubectl rollout restart deployment/auth-service
        kubectl rollout restart deployment/chat-service
        kubectl rollout restart deployment/frontend
      